{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BotUd5dr9jbN",
        "outputId": "b5059c7c-e7dd-44fa-c74e-bd25bd77225e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rk14vXbvqDVr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import onnx\n",
        "import onnxruntime as ort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WfelQ_sM8Yk1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Hyperparameters ---\n",
        "ONNX_MODEL_PATH = \"lstm_food_freshness.onnx\"\n",
        "DATA_FILE = \"https://raw.githubusercontent.com/PenyelamatPangan/Models/main/food_freshness_dataset.csv\"\n",
        "\n",
        "\n",
        "SEQUENCE_LENGTH = 10  # How many time steps to look back\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30 # MODIFIED: Increased epochs\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_SIZE = 32      # MODIFIED: Simplified model\n",
        "NUM_LAYERS = 1        # MODIFIED: Simplified model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PoeB794p8dKk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 2. Helper Function to Create Sequences ---\n",
        "def create_sequences(features, labels, seq_length):\n",
        "    \"\"\"\n",
        "    Creates sliding window sequences from tabular data.\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - seq_length):\n",
        "        X_seq.append(features[i:(i + seq_length)])\n",
        "        y_seq.append(labels[i + seq_length - 1]) # Label is from the last item in the window\n",
        "    return np.array(X_seq), np.array(y_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrcN5Usr8jDZ",
        "outputId": "cf4eff2c-484c-4517-a20d-1fe3d94aff8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting LSTM training and ONNX export...\n",
            "Loaded 100000 rows from https://raw.githubusercontent.com/PenyelamatPangan/Models/main/food_freshness_dataset.csv.\n",
            "Created 99990 sequences of length 10.\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting LSTM training and ONNX export...\")\n",
        "\n",
        "# --- 3. Load and Preprocess Data ---\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{DATA_FILE}' not found.\")\n",
        "    print(\"Please run the data generation script from the previous step first.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loaded {len(df)} rows from {DATA_FILE}.\")\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "features = ['C2H5OH_PPM', 'NH3_PPM', 'CH4_PPM', 'MQ_Analog_Value']\n",
        "target = 'Output'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Create sequences\n",
        "# This is the most important step for an LSTM\n",
        "X_seq, y_seq = create_sequences(X_scaled, y, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Created {len(X_seq)} sequences of length {SEQUENCE_LENGTH}.\")\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_seq, X_test_seq, y_train, y_test = train_test_split(\n",
        "    X_seq, y_seq, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "# Shape required for LSTM: (batch_size, sequence_length, num_features)\n",
        "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToIZd0108qp0",
        "outputId": "992d9a38-58d3-4be9-afe0-b39e01bcc22b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 4. Define the LSTM Model ---\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM layer\n",
        "        # batch_first=True makes input shape (batch_size, seq_length, input_size)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=0.2)\n",
        "\n",
        "        # Fully connected layer to map LSTM output to our desired output\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Sigmoid for final probability\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        # (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        # out shape: (batch_size, seq_length, hidden_size)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # We only care about the output of the *last* time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass last output through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Apply sigmoid for classification\n",
        "        # Note: For ONNX export, we include sigmoid in the model.\n",
        "        # For training, BCEWithLogitsLoss is better (it has sigmoid built-in).\n",
        "        if not self.training:\n",
        "            out = self.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "input_size = len(features)  # 4 features\n",
        "model = LSTMClassifier(input_size, HIDDEN_SIZE, NUM_LAYERS)\n",
        "criterion = nn.BCEWithLogitsLoss() # Numerically stable, includes sigmoid\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH3aV6Wu8t0U",
        "outputId": "6ea72760-d2af-4e5e-c59c-96cab8aa4604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 30 epochs...\n",
            "Epoch [1/30], Loss: 0.0963\n",
            "Epoch [2/30], Loss: 0.0139\n",
            "Epoch [3/30], Loss: 0.0089\n",
            "Epoch [4/30], Loss: 0.0050\n",
            "Epoch [5/30], Loss: 0.0023\n",
            "Epoch [6/30], Loss: 0.0010\n",
            "Epoch [7/30], Loss: 0.0005\n",
            "Epoch [8/30], Loss: 0.0002\n",
            "Epoch [9/30], Loss: 0.0001\n",
            "Epoch [10/30], Loss: 0.0001\n",
            "Epoch [11/30], Loss: 0.0000\n",
            "Epoch [12/30], Loss: 0.0000\n",
            "Epoch [13/30], Loss: 0.0000\n",
            "Epoch [14/30], Loss: 0.0000\n",
            "Epoch [15/30], Loss: 0.0000\n",
            "Epoch [16/30], Loss: 0.0000\n",
            "Epoch [17/30], Loss: 0.0000\n",
            "Epoch [18/30], Loss: 0.0000\n",
            "Epoch [19/30], Loss: 0.0000\n",
            "Epoch [20/30], Loss: 0.0000\n",
            "Epoch [21/30], Loss: 0.0000\n",
            "Epoch [22/30], Loss: 0.0000\n",
            "Epoch [23/30], Loss: 0.0000\n",
            "Epoch [24/30], Loss: 0.0000\n",
            "Epoch [25/30], Loss: 0.0000\n",
            "Epoch [26/30], Loss: 0.0000\n",
            "Epoch [27/30], Loss: 0.0000\n",
            "Epoch [28/30], Loss: 0.0000\n",
            "Epoch [29/30], Loss: 0.0000\n",
            "Epoch [30/30], Loss: 0.0000\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 5. Train the Model ---\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (seqs, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(seqs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR6lrlNj8xoC",
        "outputId": "536a734d-6c97-4ab2-96a5-9f7aa76c3e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy: 99.86%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Bad (0)       1.00      1.00      1.00      9964\n",
            "   Fresh (1)       1.00      1.00      1.00     10034\n",
            "\n",
            "    accuracy                           1.00     19998\n",
            "   macro avg       1.00      1.00      1.00     19998\n",
            "weighted avg       1.00      1.00      1.00     19998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 6. Evaluate the Model ---\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for seqs, labels in test_loader:\n",
        "        # Get raw logit outputs\n",
        "        outputs = model(seqs)\n",
        "\n",
        "        # Apply sigmoid and threshold to get 0 or 1\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Bad (0)', 'Fresh (1)']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBRwNTvV8z0B",
        "outputId": "02d54979-498f-4f9a-aa3d-af853526797e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exporting model to lstm_food_freshness.onnx...\n",
            "Successfully exported model to lstm_food_freshness.onnx\n",
            "Verifying ONNX model...\n",
            "ONNX model verification successful. Inference test output shape: (1, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1477254319.py:13: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 7. Export to ONNX ---\n",
        "print(f\"\\nExporting model to {ONNX_MODEL_PATH}...\")\n",
        "\n",
        "# Set model to evaluation mode (important for ONNX)\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input matching the model's input shape\n",
        "# (batch_size, sequence_length, num_features)\n",
        "# We use a batch size of 1 for the dummy input\n",
        "dummy_input = torch.randn(1, SEQUENCE_LENGTH, input_size, requires_grad=True)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(\n",
        "    model,                          # The model to export\n",
        "    dummy_input,                    # A dummy input\n",
        "    ONNX_MODEL_PATH,                # Where to save the model\n",
        "    export_params=True,             # Store the trained weights\n",
        "    opset_version=11,               # ONNX version\n",
        "    do_constant_folding=True,       # Optimization\n",
        "    input_names=['input_sequence'], # Name for the input\n",
        "    output_names=['output'],        # Name for the output\n",
        "    dynamic_axes={                  # --- VERY IMPORTANT ---\n",
        "        'input_sequence': {0: 'batch_size'}, # Allows variable batch size\n",
        "        'output': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Successfully exported model to {ONNX_MODEL_PATH}\")\n",
        "\n",
        "# (Optional) Verify the ONNX model\n",
        "print(\"Verifying ONNX model...\")\n",
        "onnx_model = onnx.load(ONNX_MODEL_PATH)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Test with ONNX Runtime\n",
        "ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.detach().numpy()}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "print(\"ONNX model verification successful. Inference test output shape:\", ort_outs[0].shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
